{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM AI Workflow Specialization Capstone #\n",
    "## Assimilating The Business Opportunity And Articulate Testable Hypotheses ##\n",
    "\n",
    "The management at AAVAIL wants to know whether their alternative approaches to generating revenue have ultimately been successful or not. They have a sizable amount of data over a reasonable amount of time, but aren't quite sure what to make of said data. They want a predictive model that will be able to predict the revenue for the following month, and additionally they want the ability for the service to predict projected revenue for specific countries in the dataset. \n",
    "\n",
    "The results are expected to be more accurate than the current model that they are using. \n",
    "\n",
    "### Potential hypotheses ###\n",
    "\n",
    "There are a number of potential hypotheses that could be tested. It ultimately depends on the quality and complexity of the data that was collected. \n",
    "<ul>\n",
    "<li>The amount of revenue generated for each month can be modelled as a Bayesian model, where the revenue for each month is dependent on the revenue of the previous month(s) and several other factors.</li>\n",
    "<li>Revenue per month can also be correlated with the amount of content streamed per each individual user and average time of engagement with content on the platform in the previous month(s).</li>\n",
    "<li>Revenue per month can also be correlated with the average amount of unique content streamed per each individual user in each of the previous month(s).</li>\n",
    "</ul>\n",
    "\n",
    "### Data needed to address each hypothesis ###\n",
    "\n",
    "Ultimately the data that we need to address each of the hypotheses are as follows:\n",
    "\n",
    "<ul>\n",
    "<li>For modelling the amount of revenue generated for each month, we need the time of each stream aggregated by month and the total amount of money spent per invoice per month.</li>\n",
    "<li>Modelling the amount of content streamed per each individual user simply requires counting all of the entries in a database and grouping them by user_id.</li>\n",
    "<li>Calculating the average time of engagement with content on the platform is highly dependent on the amount of data that we have available. A simple start and end times for each stream might suffice, but in the case where that data isn't available, a good fallback would be the length/runtime of each piece of content that the user purchased.</li>\n",
    "<li>Calculating the average amount of unique content streamed per each individual user simply requires aggregating over the dataset and grouping by the content_id and the user_id.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting load_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile load_data.py\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "def load_data(pathname):\n",
    "#load data\n",
    "    data = []\n",
    "    for filename in glob.glob(f\"{pathname}/*.json\"):\n",
    "        with open(filename, \"r\") as file:\n",
    "            for line in file:\n",
    "                entry = json.loads(line)\n",
    "                data += entry\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do a bit of EDA on the data before we can \n",
    "import pandas as pd\n",
    "from load_data import load_data\n",
    "DATA_DIR = \"cs-train\"\n",
    "df = load_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several missing stream_ids even though there are two columns both with the name StreamID. While this isn't the end of the world, it does give pause as to see what other things are missing in the dataset. \n",
    "\n",
    "We are also missing quite a number of customer_ids as well, which isn't a bad thing since we can simply reconstruct the number of users based on each individual invoice. It is a big deal, however, in terms of the hypotheses I had proposed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['country', 'customer_id', 'invoice', 'price', 'stream_id',\n",
      "       'times_viewed', 'year', 'month', 'day', 'total_price', 'StreamID',\n",
      "       'TimesViewed'],\n",
      "      dtype='object')\n",
      "292297\n",
      "815011\n",
      "189762\n",
      "['489597' '489596' '489642' ... '562134' '562135' '562164']\n",
      "522714\n",
      "Series([], Name: times_viewed, dtype: float64)\n",
      "45228      1.0\n",
      "45229      1.0\n",
      "45230     12.0\n",
      "45231      1.0\n",
      "45232      1.0\n",
      "          ... \n",
      "567937     2.0\n",
      "567938     2.0\n",
      "567939     1.0\n",
      "567940     5.0\n",
      "567941     1.0\n",
      "Name: TimesViewed, Length: 522714, dtype: float64\n",
      "               country  customer_id invoice  price stream_id  times_viewed  \\\n",
      "0       United Kingdom      13085.0  489434   6.95       NaN           NaN   \n",
      "1       United Kingdom          NaN  489597   8.65       NaN           NaN   \n",
      "2       United Kingdom          NaN  489597   1.70       NaN           NaN   \n",
      "3       United Kingdom          NaN  489597   1.70       NaN           NaN   \n",
      "4       United Kingdom          NaN  489597   0.87       NaN           NaN   \n",
      "...                ...          ...     ...    ...       ...           ...   \n",
      "815006  United Kingdom      15628.0  562163   0.85       NaN           NaN   \n",
      "815007  United Kingdom      15628.0  562163   1.95       NaN           NaN   \n",
      "815008  United Kingdom      15628.0  562163   0.83       NaN           NaN   \n",
      "815009  United Kingdom      15628.0  562163   4.95       NaN           NaN   \n",
      "815010  United Kingdom      15628.0  562163   1.65       NaN           NaN   \n",
      "\n",
      "        year month day  total_price StreamID  TimesViewed  \n",
      "0       2017    11  28          NaN      NaN          NaN  \n",
      "1       2017    11  28          NaN      NaN          NaN  \n",
      "2       2017    11  28          NaN      NaN          NaN  \n",
      "3       2017    11  28          NaN      NaN          NaN  \n",
      "4       2017    11  28          NaN      NaN          NaN  \n",
      "...      ...   ...  ..          ...      ...          ...  \n",
      "815006  2019    07  31          NaN      NaN          NaN  \n",
      "815007  2019    07  31          NaN      NaN          NaN  \n",
      "815008  2019    07  31          NaN      NaN          NaN  \n",
      "815009  2019    07  31          NaN      NaN          NaN  \n",
      "815010  2019    07  31          NaN      NaN          NaN  \n",
      "\n",
      "[292297 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#get the number of columns where both stream id columns are empty\n",
    "print(df.columns)\n",
    "print(len(df[(df[\"stream_id\"].isna())&(df[\"StreamID\"].isna())]))\n",
    "#we have the full price data for each purchase in the dataset\n",
    "print(len(df[(~df[\"total_price\"].isna())|(~df[\"price\"].isna())]))\n",
    "#no for the most concerning bit: how many customer ids are there that are missing?\n",
    "print(len(df[df[\"customer_id\"].isna()]))\n",
    "\n",
    "#however, there's another interesting question to ask: how many unique invoice_ids are there that have customer ids?\n",
    "print(df[(df[\"customer_id\"].isna())][\"invoice\"].unique())\n",
    "\n",
    "print(len(df[(~df[\"times_viewed\"].isna())|(~df[\"TimesViewed\"].isna())]))\n",
    "print(df[~df[\"times_viewed\"].isna()][\"times_viewed\"])\n",
    "print(df[~df[\"TimesViewed\"].isna()][\"TimesViewed\"])\n",
    "df[\"times_viewed\"] = df.apply(lambda x: x[\"TimesViewed\"] if x[\"TimesViewed\"] != \"NaN\" else x[\"times_viewed\"], axis=1)\n",
    "print(df[df[\"times_viewed\"].isna()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
